{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with reading the section “Implementing MLPs with Keras” from Chapter 10 of Geron’s text-book (pages 292-325).\n",
    "Then install `TensorFlow 2.0+` and experiment with the code included in this section.\n",
    "Additionally, study the official documentation (https://keras.io/) and get an idea of the numerous options offered by Keras (layers, loss functions, metrics, optimizers, activations, initializers, regularizers).\n",
    "Don’t get overwhelmed with the number of options – you will frequently return to this site in the coming months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "from functools import partial\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "\n",
    "from utils import get_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRS = get_dirs(os.path.abspath('') + os.sep + 'Task1.ipynb')\n",
    "print('\\033[1m' + 'Directories:' + '\\033[0m')\n",
    "for dir_name, path in DIRS.items():\n",
    "    print(f'{dir_name:<7} {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this official repository with many examples of Keras implementations of various sorts of deep neural networks [here](https://github.com/keras-team/keras/tree/tf-keras-2/examples).\n",
    "We recommend cloning this repository and try to get some of these examples running on your system (or Colab/DeepNote).\n",
    "In particular, experiment with `mnist_mlp.py` and `mnist_cnn.py` scripts which show you how to build simple neural networks for the MNIST dataset (useful for the next task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*insert findings*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, take the two well-known datasets: Fashion MNIST (introduced in _Ch. 10, p. 295_) and CIFAR-10.\n",
    "The first dataset contains 2D (grayscale) images of size 28x28, split into 10 categories; 60,000 images for training and 10,000 for testing, while the latter contains 32x32x3 RGB images (50,000/10,000 train/test).\n",
    "Apply two reference networks on the fashion MNIST dataset: a MLP described in detail in _Ch. 10, pp. 297-307_ and a CNN described in _Ch. 14, p. 447_.\n",
    "Experiment with both networks, trying various options: initializations, activations, optimizers (and their hyperparameters), regularizations (L1, L2, Dropout, no Dropout).\n",
    "You may also experiment with changing the architecture of both networks: adding/removing layers, number of convolutional filters, their sizes, etc.\n",
    "\n",
    "After you have found the best performing hyperparameter sets, take the 3 best ones and train new models on the CIFAR-10 dataset, see whether your performance gains translate to a different dataset.\n",
    "Provide your thoughts on these results in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a MLP model for the fashion MNIST dataset.\n",
    "We use the same model as in the book, but we add a dropout layer after the first dense layer.\n",
    "We also use the Adam optimizer with a learning rate of 0.001.\n",
    "We train the model for 10 epochs and use a batch size of 32.\n",
    "We use the same model for the CIFAR-10 dataset, but we change the number of epochs to 20 and the batch size to 64.\n",
    "We also use a learning rate of 0.001 for the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test different hyperparameters for a 2-layer MLP as defined in chapter 10 of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['optimizer', 'lr', 'activation', 'loss', 'accuracy', 'traintime'])\n",
    "reps = 3\n",
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "lrs = [1e-3, 5e-3, 1e-2]\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "configs = list(product(optimizers, lrs, activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 1\n",
    "for rep in range(reps):\n",
    "    for optimizer, lr, activation in configs:\n",
    "        print(f'\\r{run}/{len(configs)*reps}', end='')\n",
    "        model = Sequential()\n",
    "        model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "        model.add(layers.Dense(300, activation=activation))\n",
    "        model.add(layers.Dense(100, activation=activation))\n",
    "        model.add(layers.Dense(10, activation='softmax'))\n",
    "        model.compile(\n",
    "            optimizer = optimizer(learning_rate=lr),\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['accuracy']\n",
    "        )\n",
    "        tic = perf_counter()\n",
    "        history = model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "        toc = perf_counter()\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "        df.loc[f'{optimizer.__name__}-{activation}-{lr}'] = [\n",
    "            optimizer.__name__,\n",
    "            lr,\n",
    "            activation,\n",
    "            test_loss / reps,\n",
    "            test_acc / reps,\n",
    "            (toc-tic) / reps\n",
    "        ]\n",
    "        run += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check if the dataframe indeed contains the data we're interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(DIRS['csv'] + 'mlp_hpo_42.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for i, optimizer in enumerate([opt.__name__ for opt in optimizers]):\n",
    "    for j, metric in enumerate(['accuracy', 'loss', 'traintime']):\n",
    "        ax = axes[i,j]\n",
    "        for activation in activations:\n",
    "            df[(df.optimizer == optimizer) & (df.activation == activation)].plot(\n",
    "                x = 'lr',\n",
    "                y = metric,\n",
    "                ax = ax,\n",
    "                label = activation\n",
    "            )\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(optimizer) if j == 0 else ax.set_ylabel('')\n",
    "        ax.set_xticks(lrs, fontsize=3)\n",
    "        ax.get_legend().remove()\n",
    "        ax.set_title(metric) if i == 0 else ax.set_title('')\n",
    "\n",
    "# set global legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.99, 0.99), ncol=3, fontsize=14)\n",
    "\n",
    "fig.suptitle('MLP - Fashion MNIST HP exploration', fontsize=20, weight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('../results/plots/mlp_hpo_42.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these results, we can not point to any clear winners; sometimes a relatively low learning rate is preferable, sometimes a high learning rate performs better.\n",
    "The same goes for the activation functions, as well as (to some extent) for the optimizers.\n",
    "What we can say, however, is that the Adam and RMSprop optimizers seem to prefer a lower learning rate, while the SGD optimizer seems to prefer a higher learning rate.\n",
    "This makes sense because the Adam and RMSprop optimizers are adaptive optimizers, while the SGD optimizer is not.\n",
    "Another observation is that RMSprop is relatively slow to converge, while Adam and SGD are relatively fast to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, activation='relu', padding='SAME')\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer=\"nadam\",\n",
    "metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "# TODO fix MLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 ('python3.8env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ba171ecc2aa3d0e9e99a3b536fabfab27933c0b10ad7d2c9bd2c5d6b65c7445d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
