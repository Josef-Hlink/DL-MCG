{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with reading the section “Implementing MLPs with Keras” from Chapter 10 of Geron’s text-book (pages 292-325).\n",
    "Then install `TensorFlow 2.0+` and experiment with the code included in this section.\n",
    "Additionally, study the official documentation (https://keras.io/) and get an idea of the numerous options offered by Keras (layers, loss functions, metrics, optimizers, activations, initializers, regularizers).\n",
    "Don’t get overwhelmed with the number of options – you will frequently return to this site in the coming months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import os\n",
    "from itertools import product\n",
    "from time import perf_counter\n",
    "from typing import Callable\n",
    "\n",
    "# pip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "\n",
    "# local\n",
    "from utils import get_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRS = get_dirs(os.path.abspath('') + os.sep + 'Task1.ipynb')\n",
    "print('\\033[1m' + 'Directories:' + '\\033[0m')\n",
    "for dir_name, path in DIRS.items():\n",
    "    print(f'{dir_name:<7} {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this official repository with many examples of Keras implementations of various sorts of deep neural networks [here](https://github.com/keras-team/keras/tree/tf-keras-2/examples).\n",
    "We recommend cloning this repository and try to get some of these examples running on your system (or Colab/DeepNote).\n",
    "In particular, experiment with `mnist_mlp.py` and `mnist_cnn.py` scripts which show you how to build simple neural networks for the MNIST dataset (useful for the next task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*insert findings*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, take the two well-known datasets: Fashion MNIST (introduced in _Ch. 10, p. 295_) and CIFAR-10.\n",
    "The first dataset contains 2D (grayscale) images of size 28x28, split into 10 categories; 60,000 images for training and 10,000 for testing, while the latter contains 32x32x3 RGB images (50,000/10,000 train/test).\n",
    "Apply two reference networks on the fashion MNIST dataset: a MLP described in detail in _Ch. 10, pp. 297-307_ and a CNN described in _Ch. 14, p. 447_.\n",
    "Experiment with both networks, trying various options: initializations, activations, optimizers (and their hyperparameters), regularizations (L1, L2, Dropout, no Dropout).\n",
    "You may also experiment with changing the architecture of both networks: adding/removing layers, number of convolutional filters, their sizes, etc.\n",
    "\n",
    "After you have found the best performing hyperparameter sets, take the 3 best ones and train new models on the CIFAR-10 dataset, see whether your performance gains translate to a different dataset.\n",
    "Provide your thoughts on these results in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a MLP model for the fashion MNIST dataset.\n",
    "We use the same model as in the book, but we add a dropout layer after the first dense layer.\n",
    "We also use the Adam optimizer with a learning rate of 0.001.\n",
    "We train the model for 10 epochs and use a batch size of 32.\n",
    "We use the same model for the CIFAR-10 dataset, but we change the number of epochs to 20 and the batch size to 64.\n",
    "We also use a learning rate of 0.001 for the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load FMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_f, y_train_f), (X_test_f, y_test_f) = fashion_mnist.load_data()\n",
    "\n",
    "X_train_f = X_train_f.astype('float32') / 255\n",
    "X_test_f = X_test_f.astype('float32') / 255\n",
    "\n",
    "X_train_f.shape, X_test_f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define functions for running the hyperparameter exploration experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_default_MLP(input_shape, activation, optimizer, lr) -> Sequential:\n",
    "    \"\"\"\n",
    "    Returns a compiled default MLP classifier architecture with\n",
    "    a given input shape, activation function, optimizer and learning rate.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dense(300, activation=activation),\n",
    "        layers.Dense(100, activation=activation),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = optimizer(learning_rate=lr),\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_default_CNN(input_shape, activation, optimizer, lr) -> Sequential:\n",
    "    \"\"\"\n",
    "    Returns a compiled default CNN classifier architecture with\n",
    "    a given input shape, activation function, optimizer and learning rate.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=activation, input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=activation),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=activation),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=activation),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = optimizer(learning_rate=lr),\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model_constructor: Callable, datasets: tuple, configs: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        model_constructor (function) - build_default_MLP or build_default_CNN\n",
    "        datasets (tuple) - (X_train, y_train, X_test, y_test)\n",
    "        configs (list) - list of tuples of (optimizer, lr, activation)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    X_train, y_train, X_test, y_test = datasets\n",
    "    if 'CNN' in model_constructor.__name__:\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['optimizer', 'lr', 'activation', 'loss', 'accuracy', 'traintime'])\n",
    "    run = 1\n",
    "    for optimizer, lr, activation in configs:\n",
    "        losses, accuracies, traintimes = [], [], []\n",
    "        for _ in range(3):\n",
    "            print(f'\\r{run}/{len(configs)*3}', end='')\n",
    "            \n",
    "            model = model_constructor(\n",
    "                input_shape = X_train.shape[1:],\n",
    "                activation = activation,\n",
    "                optimizer = optimizer,\n",
    "                lr = lr\n",
    "            )\n",
    "            \n",
    "            tic = perf_counter()\n",
    "            model.fit(\n",
    "                x = X_train,\n",
    "                y = y_train,\n",
    "                epochs = 5,\n",
    "                batch_size = 64,\n",
    "                verbose = 0\n",
    "            )\n",
    "            toc = perf_counter()\n",
    "\n",
    "            test_loss, test_acc = model.evaluate(\n",
    "                x = X_test,\n",
    "                y = y_test,\n",
    "                verbose = 0\n",
    "            )\n",
    "            \n",
    "            losses.append(test_loss)\n",
    "            accuracies.append(test_acc)\n",
    "            traintimes.append(toc-tic)\n",
    "            run += 1\n",
    "\n",
    "        df.loc[f'{optimizer.__name__}-{activation}-{lr}'] = [\n",
    "            optimizer.__name__,\n",
    "            lr,\n",
    "            activation,\n",
    "            np.mean(losses),\n",
    "            np.mean(accuracies),\n",
    "            np.mean(traintimes)\n",
    "        ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(df, optimizers, activations, lrs, title) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a 3x3 grid of plots for the given optimizers, activations and learning rates.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    for i, optimizer in enumerate([opt.__name__ for opt in optimizers]):\n",
    "        for j, metric in enumerate(['accuracy', 'loss', 'traintime']):\n",
    "            ax = axes[i,j]\n",
    "            for activation in activations:\n",
    "                df[(df.optimizer == optimizer) & (df.activation == activation)].plot(\n",
    "                    x = 'lr',\n",
    "                    y = metric,\n",
    "                    ax = ax,\n",
    "                    label = activation\n",
    "                )\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel(optimizer) if j == 0 else ax.set_ylabel('')\n",
    "            ax.set_xticks(lrs, fontsize=3)\n",
    "            ax.get_legend().remove()\n",
    "            ax.set_title(metric) if i == 0 else ax.set_title('')\n",
    "\n",
    "    # set global legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.99, 0.99), ncol=3, fontsize=14)\n",
    "    fig.suptitle(title, fontsize=20, weight='bold')\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test different hyperparameters for a 2-hidden-layer MLP as defined in chapter 10 of the book (FMNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "lrs = [1e-3, 5e-3, 1e-2]\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "configs = list(product(optimizers, lrs, activations))\n",
    "\n",
    "df_mf = run_experiment(build_default_MLP, (X_train_f, y_train_f, X_test_f, y_test_f), configs)\n",
    "df_mf.to_csv(DIRS['csv'] + 'mlp_fmnist.csv', index=False)\n",
    "fig_mf = create_plots(df_mf, optimizers, activations, lrs, 'Fashion MNIST MLP')\n",
    "fig_mf.savefig(DIRS['plots'] + 'mlp_fmnist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "do the same for a 3-hidden-layer CNN as defined in chapter 14 of the book with some modifications to save on runtime (FMNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the same configs list as for the MLP\n",
    "\n",
    "df_cf = run_experiment(build_default_CNN, (X_train_f, y_train_f, X_test_f, y_test_f), configs)\n",
    "df_cf.to_csv(DIRS['csv'] + 'cnn_fmnist.csv', index=False)\n",
    "fig_cf = create_plots(df_cf, optimizers, activations, lrs, 'Fashion MNIST CNN')\n",
    "fig_cf.savefig(DIRS['plots'] + 'cnn_fmnist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we run the same two experiments, but on the CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_f, y_train_f, X_test_f, y_test_f\n",
    "\n",
    "(X_train_c, y_train_c), (X_test_c, y_test_c) = cifar10.load_data()\n",
    "\n",
    "X_train_c = X_train_c.astype('float32') / 255\n",
    "X_test_c = X_test_c.astype('float32') / 255\n",
    "\n",
    "X_train_c.shape, X_test_c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mc = run_experiment(build_default_MLP, (X_train_c, y_train_c, X_test_c, y_test_c), configs)\n",
    "df_mc.to_csv(DIRS['csv'] + 'mlp_cifar.csv', index=False)\n",
    "fig_mc = create_plots(df_mc, optimizers, activations, lrs, 'CIFAR-10 MLP')\n",
    "fig_mc.savefig(DIRS['plots'] + 'mlp_cifar.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cc = run_experiment(build_default_CNN, (X_train_c, y_train_c, X_test_c, y_test_c), configs)\n",
    "df_cc.to_csv(DIRS['csv'] + 'cnn_cifar.csv', index=False)\n",
    "fig_cc = create_plots(df_cc, optimizers, activations, lrs, 'CIFAR-10 CNN')\n",
    "fig_cc.savefig(DIRS['plots'] + 'cnn_cifar.png', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot y_pred vs y_true\n",
    "def plot_scatter(y_true, y_pred, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ax.scatter(y_true, y_pred, s=1)\n",
    "    ax.set_xlabel('y_true')\n",
    "    ax.set_ylabel('y_pred')\n",
    "    ax.set_title(title)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73a50edc90de9e6c73e4de59c0149da437672aa3b7b7e4f31799c5ff4b4c231b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
