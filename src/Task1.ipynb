{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with reading the section “Implementing MLPs with Keras” from Chapter 10 of Geron’s text-book (pages 292-325).\n",
    "Then install `TensorFlow 2.0+` and experiment with the code included in this section.\n",
    "Additionally, study the official documentation (https://keras.io/) and get an idea of the numerous options offered by Keras (layers, loss functions, metrics, optimizers, activations, initializers, regularizers).\n",
    "Don’t get overwhelmed with the number of options – you will frequently return to this site in the coming months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from time import perf_counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out this official repository with many examples of Keras implementations of various sorts of deep neural networks [here](https://github.com/keras-team/keras/tree/tf-keras-2/examples).\n",
    "We recommend cloning this repository and try to get some of these examples running on your system (or Colab/DeepNote).\n",
    "In particular, experiment with `mnist_mlp.py` and `mnist_cnn.py` scripts which show you how to build simple neural networks for the MNIST dataset (useful for the next task)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*insert findings*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, take the two well-known datasets: Fashion MNIST (introduced in _Ch. 10, p. 295_) and CIFAR-10.\n",
    "The first dataset contains 2D (grayscale) images of size 28x28, split into 10 categories; 60,000 images for training and 10,000 for testing, while the latter contains 32x32x3 RGB images (50,000/10,000 train/test).\n",
    "Apply two reference networks on the fashion MNIST dataset: a MLP described in detail in _Ch. 10, pp. 297-307_ and a CNN described in _Ch. 14, p. 447_.\n",
    "Experiment with both networks, trying various options: initializations, activations, optimizers (and their hyperparameters), regularizations (L1, L2, Dropout, no Dropout).\n",
    "You may also experiment with changing the architecture of both networks: adding/removing layers, number of convolutional filters, their sizes, etc.\n",
    "\n",
    "After you have found the best performing hyperparameter sets, take the 3 best ones and train new models on the CIFAR-10 dataset, see whether your performance gains translate to a different dataset.\n",
    "Provide your thoughts on these results in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create a MLP model for the fashion MNIST dataset.\n",
    "We use the same model as in the book, but we add a dropout layer after the first dense layer.\n",
    "We also use the Adam optimizer with a learning rate of 0.001.\n",
    "We train the model for 10 epochs and use a batch size of 32.\n",
    "We use the same model for the CIFAR-10 dataset, but we change the number of epochs to 20 and the batch size to 64.\n",
    "We also use a learning rate of 0.0001 for the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test different hyperparameters for a 2-layer MLP as defined in chapter 10 of the book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['optimizer', 'lr', 'activation', 'loss', 'accuracy', 'traintime'])\n",
    "reps = 3\n",
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "lrs = [1e-3, 5e-3, 1e-2]\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "\n",
    "configs = list(product(optimizers, lrs, activations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = 1\n",
    "for rep in range(reps):\n",
    "    for optimizer, lr, activation in configs:\n",
    "        print(f'\\r{run}/{len(configs)*reps}', end='')\n",
    "        model = Sequential()\n",
    "        model.add(layers.Flatten(input_shape=(28, 28)))\n",
    "        model.add(layers.Dense(300, activation=activation))\n",
    "        model.add(layers.Dense(100, activation=activation))\n",
    "        model.add(layers.Dense(10, activation='softmax'))\n",
    "        model.compile(\n",
    "            optimizer = optimizer(learning_rate=lr),\n",
    "            loss = 'sparse_categorical_crossentropy',\n",
    "            metrics = ['accuracy']\n",
    "        )\n",
    "        tic = perf_counter()\n",
    "        history = model.fit(x_train, y_train, epochs=5, batch_size=64, verbose=0)\n",
    "        toc = perf_counter()\n",
    "        test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
    "        df.loc[f'{optimizer.__name__}-{activation}-{lr}'] = [\n",
    "            optimizer.__name__,\n",
    "            lr,\n",
    "            activation,\n",
    "            test_loss / reps,\n",
    "            test_acc / reps,\n",
    "            (toc-tic) / reps\n",
    "        ]\n",
    "        run += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "for i, optimizer in enumerate([opt.__name__ for opt in optimizers]):\n",
    "    for j, metric in enumerate(['accuracy', 'loss', 'traintime']):\n",
    "        ax = axes[i,j]\n",
    "        sns.lineplot(x='lr', y=metric, hue='activation', data=df[df['optimizer'] == optimizer], ax=ax)\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel(optimizer) if j == 0 else ax.set_ylabel('')\n",
    "        ax.set_xticks(lrs, fontsize=3)\n",
    "        ax.get_legend().remove()\n",
    "        ax.set_title(metric) if i == 0 else ax.set_title('')\n",
    "\n",
    "# set global legend\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.99, 0.99), ncol=3, fontsize=14)\n",
    "\n",
    "fig.suptitle('MLP - Fashion MNIST HPO exploration', fontsize=20, weight='bold')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig('../plots/MLP_hpo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_to be edited!_\n",
    "\n",
    "ReLU seems to be the best activation function, and the Adam optimizer with a learning rate of 0.001 seems to be the best optimizer.\n",
    "ReLU does not only achieve the lowest loss and highest accuracy, but it is also faster to train with than the other activation functions.\n",
    "\n",
    "Between sigmoid and tanh, sigmoid takes the cake while having practically identical training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "DefaultConv2D = partial(keras.layers.Conv2D,\n",
    "kernel_size=3, activation='relu', padding=\"SAME\")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    DefaultConv2D(filters=64, kernel_size=7, input_shape=[28, 28, 1]),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=128),\n",
    "    DefaultConv2D(filters=128),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    DefaultConv2D(filters=256),\n",
    "    DefaultConv2D(filters=256),\n",
    "    keras.layers.MaxPooling2D(pool_size=2),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(units=128, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=64, activation='relu'),\n",
    "    keras.layers.Dropout(0.5),\n",
    "    keras.layers.Dense(units=10, activation='softmax'),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "optimizer=\"nadam\",\n",
    "metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(x_train, y_train, epochs=5, batch_size=128, validation_data=(x_test, y_test))\n",
    "\n",
    "model.evaluate(x_test, y_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73a50edc90de9e6c73e4de59c0149da437672aa3b7b7e4f31799c5ff4b4c231b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
