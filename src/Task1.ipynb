{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1: Learn the basics of Keras API for TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with reading the section “Implementing MLPs with Keras” from _Chapter 10 of Geron’s text-book (pages 292-325)_.\n",
    "Then install `TensorFlow 2.0+` and experiment with the code included in this section.\n",
    "Additionally, study the official documentation (https://keras.io/) and get an idea of the numerous options offered by Keras (layers, loss functions, metrics, optimizers, activations, initializers, regularizers).\n",
    "Don’t get overwhelmed with the number of options – you will frequently return to this site in the coming months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stdlib\n",
    "import os\n",
    "from itertools import product\n",
    "from time import perf_counter\n",
    "from typing import Callable\n",
    "\n",
    "# pip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.datasets import fashion_mnist, cifar10\n",
    "\n",
    "# local\n",
    "from utils import get_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRS = get_dirs(os.path.abspath('') + os.sep + 'Task1.ipynb')\n",
    "print('\\033[1m' + 'Directories:' + '\\033[0m')\n",
    "for dir_name, path in DIRS.items():\n",
    "    print(f'{dir_name:<7} {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first test multiple sets of hyperparameters on the MNIST dataset using the MLP and the CNN model from the book. We will extract the top 3 best performing sets from this experiment and use them to train a MLP and a CNN on the CIFAR-10 dataset, to have a look how well these hyperparameters generalize to a different dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, load FMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_f, y_train_f), (X_test_f, y_test_f) = fashion_mnist.load_data()\n",
    "\n",
    "X_train_f = X_train_f.astype('float32') / 255\n",
    "X_test_f = X_test_f.astype('float32') / 255\n",
    "\n",
    "X_train_f.shape, X_test_f.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions for running the hyperparameter exploration experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_default_MLP(input_shape, activation, optimizer, lr) -> Sequential:\n",
    "    \"\"\"\n",
    "    Returns a compiled default MLP classifier architecture with\n",
    "    a given input shape, activation function, optimizer and learning rate.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        layers.Flatten(input_shape=input_shape),\n",
    "        layers.Dense(300, activation=activation),\n",
    "        layers.Dense(100, activation=activation),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = optimizer(learning_rate=lr),\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_default_CNN(input_shape, activation, optimizer, lr) -> Sequential:\n",
    "    \"\"\"\n",
    "    Returns a compiled default CNN classifier architecture with\n",
    "    a given input shape, activation function, optimizer and learning rate.\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation=activation, input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=activation),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation=activation),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation=activation),\n",
    "        layers.Dense(10, activation='softmax')\n",
    "    ])\n",
    "    model.compile(\n",
    "        optimizer = optimizer(learning_rate=lr),\n",
    "        loss = 'sparse_categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(model_constructor: Callable, datasets: tuple, configs: list) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Params:\n",
    "        model_constructor (function) - build_default_MLP or build_default_CNN\n",
    "        datasets (tuple) - (X_train, y_train, X_test, y_test)\n",
    "        configs (list) - list of tuples of (optimizer, lr, activation)\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    X_train, y_train, X_test, y_test = datasets\n",
    "    if 'CNN' in model_constructor.__name__:\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "    \n",
    "    df = pd.DataFrame(columns=['optimizer', 'lr', 'activation', 'loss', 'accuracy', 'traintime'])\n",
    "    run = 1\n",
    "    for optimizer, lr, activation in configs:\n",
    "        losses, accuracies, traintimes = [], [], []\n",
    "        for _ in range(3):\n",
    "            print(f'\\r{run}/{len(configs)*3}', end='')\n",
    "            \n",
    "            model = model_constructor(\n",
    "                input_shape = X_train.shape[1:],\n",
    "                activation = activation,\n",
    "                optimizer = optimizer,\n",
    "                lr = lr\n",
    "            )\n",
    "            \n",
    "            tic = perf_counter()\n",
    "            model.fit(\n",
    "                x = X_train,\n",
    "                y = y_train,\n",
    "                epochs = 5,\n",
    "                batch_size = 64,\n",
    "                verbose = 0\n",
    "            )\n",
    "            toc = perf_counter()\n",
    "\n",
    "            test_loss, test_acc = model.evaluate(\n",
    "                x = X_test,\n",
    "                y = y_test,\n",
    "                verbose = 0\n",
    "            )\n",
    "            \n",
    "            losses.append(test_loss)\n",
    "            accuracies.append(test_acc)\n",
    "            traintimes.append(toc-tic)\n",
    "            run += 1\n",
    "\n",
    "        df.loc[f'{optimizer.__name__}-{activation}-{lr}'] = [\n",
    "            optimizer.__name__,\n",
    "            lr,\n",
    "            activation,\n",
    "            np.mean(losses),\n",
    "            np.mean(accuracies),\n",
    "            np.mean(traintimes)\n",
    "        ]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_plots(df, optimizers, activations, lrs, title) -> plt.Figure:\n",
    "    \"\"\"\n",
    "    Creates a 3x3 grid of plots for the given optimizers, activations and learning rates.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "    for i, optimizer in enumerate([opt.__name__ for opt in optimizers]):\n",
    "        for j, metric in enumerate(['accuracy', 'loss', 'traintime']):\n",
    "            ax = axes[i,j]\n",
    "            for activation in activations:\n",
    "                df[(df.optimizer == optimizer) & (df.activation == activation)].plot(\n",
    "                    x = 'lr',\n",
    "                    y = metric,\n",
    "                    ax = ax,\n",
    "                    label = activation\n",
    "                )\n",
    "            ax.set_xlabel('')\n",
    "            ax.set_ylabel(optimizer) if j == 0 else ax.set_ylabel('')\n",
    "            ax.set_xticks(lrs, fontsize=3)\n",
    "            ax.get_legend().remove()\n",
    "            ax.set_title(metric) if i == 0 else ax.set_title('')\n",
    "\n",
    "    # set global legend\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(0.99, 0.99), ncol=3, fontsize=14)\n",
    "    fig.suptitle(title, fontsize=20, weight='bold')\n",
    "    fig.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test different hyperparameters for a 2-hidden-layer MLP as defined in chapter 10 of the book (FMNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizers = [keras.optimizers.Adam, keras.optimizers.SGD, keras.optimizers.RMSprop]\n",
    "lrs = [1e-3, 5e-3, 1e-2]\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "configs = list(product(optimizers, lrs, activations))\n",
    "\n",
    "df_mf = run_experiment(build_default_MLP, (X_train_f, y_train_f, X_test_f, y_test_f), configs)\n",
    "df_mf.to_csv(DIRS['csv'] + 'mlp_fmnist.csv', index=False)\n",
    "fig_mf = create_plots(df_mf, optimizers, activations, lrs, 'Fashion MNIST MLP')\n",
    "fig_mf.savefig(DIRS['plots'] + 'mlp_fmnist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for a 3-hidden-layer CNN as defined in chapter 14 of the book with some modifications to save on runtime (FMNIST dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the same configs list as for the MLP\n",
    "\n",
    "df_cf = run_experiment(build_default_CNN, (X_train_f, y_train_f, X_test_f, y_test_f), configs)\n",
    "df_cf.to_csv(DIRS['csv'] + 'cnn_fmnist.csv', index=False)\n",
    "fig_cf = create_plots(df_cf, optimizers, activations, lrs, 'Fashion MNIST CNN')\n",
    "fig_cf.savefig(DIRS['plots'] + 'cnn_fmnist.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now look at the 3 best performing hyperparameter sets that are described in the tables in the report and train a MLP and a CNN (the same models as used before) on the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the MNIST dataset from memory if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del X_train_f, y_train_f, X_test_f, y_test_f\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_c, y_train_c), (X_test_c, y_test_c) = cifar10.load_data()\n",
    "\n",
    "X_train_c = X_train_c.astype('float32') / 255\n",
    "X_test_c = X_test_c.astype('float32') / 255\n",
    "\n",
    "X_train_c.shape, X_test_c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the models for the 3 best performing hyperparameter sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLPmodel1 = build_default_MLP(X_train_c.shape[1:], 'sigmoid', keras.optimizers.Adam, 0.005)\n",
    "MLPmodel2 = build_default_MLP(X_train_c.shape[1:], 'relu', keras.optimizers.RMSprop, 0.001)\n",
    "MLPmodel3 = build_default_MLP(X_train_c.shape[1:], 'tanh', keras.optimizers.SGD, 0.01)\n",
    "\n",
    "CNNmodel1 = build_default_CNN(X_train_c.shape[1:], 'relu', keras.optimizers.RMSprop, 0.001)\n",
    "CNNmodel2 = build_default_CNN(X_train_c.shape[1:], 'tanh', keras.optimizers.Adam, 0.001)\n",
    "CNNmodel3 = build_default_CNN(X_train_c.shape[1:], 'tanh', keras.optimizers.SGD, 0.01)\n",
    "\n",
    "MLPmodels = [MLPmodel1, MLPmodel2, MLPmodel3]\n",
    "CNNmodels = [CNNmodel1, CNNmodel2, CNNmodel3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the models and save the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(model):\n",
    "\n",
    "    for x in range(5):\n",
    "        losses, accuracies, traintimes = [], [], []\n",
    "        historyarr = []\n",
    "        tic = perf_counter()\n",
    "        history = model.fit(\n",
    "            x = X_train_c,\n",
    "            y = y_train_c,\n",
    "            epochs = 5,\n",
    "            batch_size = 64,\n",
    "            verbose = 0\n",
    "        )\n",
    "        toc = perf_counter()\n",
    "\n",
    "        test_loss, test_acc = model.evaluate(\n",
    "            x = X_test_c,\n",
    "            y = y_test_c,\n",
    "            verbose = 1\n",
    "        )\n",
    "\n",
    "        losses.append(test_loss)\n",
    "        accuracies.append(test_acc)\n",
    "        traintimes.append(toc-tic)\n",
    "        historyarr.append(history)\n",
    "\n",
    "        return historyarr, np.mean(losses), np.mean(accuracies), np.mean(traintimes)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "historydictMLP = {}\n",
    "for model in MLPmodels:\n",
    "\n",
    "    history = trainModel(model)\n",
    "    historydictMLP[model] = history\n",
    "\n",
    "historydictCNN = {}\n",
    "for model in CNNmodels:\n",
    "\n",
    "    history = trainModel(model)\n",
    "    historydictCNN[model] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting all the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 2, figsize=(10, 10))\n",
    "\n",
    "#add the dictioarys together\n",
    "for i, model in enumerate(historydictMLP):\n",
    "\n",
    "    ax[i,0].plot(historydictMLP[model][0][0].history['loss'], label='loss')\n",
    "    twax = ax[i,0].twinx()\n",
    "    twax.plot(historydictMLP[model][0][0].history['accuracy'], color='orange', label='accuracy')\n",
    "    ax[i,0].set_title('MLP model ' + str(i+1))\n",
    "    ax[i,0].set_xlabel('Epoch')\n",
    "    ax[i,0].set_xticks(np.arange(0, 5, 1))\n",
    "    ax[i,0].set_ylabel('Loss')\n",
    "    twax.set_ylabel('Accuracy')\n",
    "    ax[i,0].legend(loc='upper left')\n",
    "    twax.legend(loc='upper right')\n",
    "\n",
    "for i, model in enumerate(historydictCNN):\n",
    "\n",
    "    ax[i,1].plot(historydictCNN[model][0][0].history['loss'], label='loss')\n",
    "    twax = ax[i,1].twinx()\n",
    "    twax.plot(historydictCNN[model][0][0].history['accuracy'], color='orange', label='accuracy')\n",
    "    ax[i,1].set_title('CNN model ' + str(i+1))\n",
    "    ax[i,1].set_xlabel('Epoch')\n",
    "    ax[i,1].set_xticks(np.arange(0, 5, 1), np.arange(0, 5, 1))\n",
    "    ax[i,1].set_ylabel('Loss')\n",
    "    ax[i,1].set_yticks(np.arange(0, 2, 0.5), np.arange(0, 2, 0.5))\n",
    "    twax.set_ylabel('Accuracy')\n",
    "    ax[i,1].legend(loc='upper left')\n",
    "    twax.legend(loc='upper right')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(DIRS['plots'] + 'cifar10.png', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73a50edc90de9e6c73e4de59c0149da437672aa3b7b7e4f31799c5ff4b4c231b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
