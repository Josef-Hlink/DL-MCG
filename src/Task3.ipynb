{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variational autoencoder (VAE) is built in a very similar way, but includes a sampling layer that gives this model its generative capabilities. The generator and discriminator in the GAN model are also build using these similar building blocks, however they are arranged differently and use a very different loss function. \n",
    " \n",
    " Your task is to find a new image dataset of your choice online and use the code provided in the notebook to retrain your generative models (both VAE and GAN). You should provide a link to the data that you used in the report along with its\n",
    "description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1\n",
    "\n",
    "We have chosen a dataset containing 10.000 images of cats. We start by importing the data from a .tag.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, Conv2DTranspose, Reshape\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing the data set we have to convert the data set to a .npy file in order to work with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load the data from the .npy file we created earlier and show a plot containing 9 random cat images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_plot(images, epoch='', name='', n=3, save=False, scale=False):\n",
    "    if scale:\n",
    "        images = (images + 1) / 2.0\n",
    "    for index in range(n * n):\n",
    "        plt.subplot(n, n, 1 + index)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(images[index])\n",
    "    fig = plt.gcf()\n",
    "    fig.suptitle(name + '  '+ str(epoch), fontsize=14)\n",
    "    if save:\n",
    "        filename = 'results/generated_plot_e%03d_f.png' % (epoch+1)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "    plt.show()\n",
    " \n",
    "def load_data(scale: bool = False):\n",
    "    X = np.load('../data/cats.npy')\n",
    "    if scale:\n",
    "        X = (X - 127.5) * 2\n",
    "    return X / 255.0\n",
    "\n",
    "\n",
    "dataset = load_data()\n",
    "grid_plot(dataset[np.random.randint(0, 1000, 9)], name='Cat dataset (64x64x3)', n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The build_conv_net() and build_deconv_net() functions are needed by both the VAE and GAN models to create a convolutional network specific for encoding and decoding purposes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_conv_net(in_shape, out_shape, n_downsampling_layers=4, filters=128, out_activation='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a basic convolutional network\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential()\n",
    "    default_args=dict(kernel_size=(3,3), strides=(2,2), padding='same', activation='relu')\n",
    "\n",
    "    model.add(Conv2D(input_shape=in_shape, **default_args, filters=filters))\n",
    "\n",
    "    for _ in range(n_downsampling_layers):\n",
    "        model.add(Conv2D(**default_args, filters=filters))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(out_shape, activation=out_activation) )\n",
    "    #model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def build_deconv_net(latent_dim, n_upsampling_layers=4, filters=128, activation_out='sigmoid'):\n",
    "    \"\"\"\n",
    "    Build a de-convolutional network for decoding/up-scaling latent vectors\n",
    "\n",
    "    - use the same layer sizes that were used in the down-sampling network. \n",
    "    - the Conv2DTranspose layers are used instead of Conv2D layers. \n",
    "    - identical layers and hyper-parameters ensures that the dimensionality of our output matches the\n",
    "      shape of our input images. \n",
    "    \"\"\"\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(4 * 4 * 64, input_dim=latent_dim)) \n",
    "    model.add(Reshape((4, 4, 64))) # This matches the output size of the downsampling architecture\n",
    "    default_args=dict(kernel_size=(3,3), strides=(2,2), padding='same', activation='relu')\n",
    "    \n",
    "    for i in range(n_upsampling_layers):\n",
    "        model.add(Conv2DTranspose(**default_args, filters=filters))\n",
    "\n",
    "    # This last convolutional layer converts back to 3 channel RGB image\n",
    "    model.add(Conv2D(filters=3, kernel_size=(3,3), activation=activation_out, padding='same'))\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Custom layer for the variational autoencoder\n",
    "    It takes two vectors as input - one for means and other for variances of the latent variables described by a multimodal gaussian\n",
    "    Its output is a latent vector randomly sampled from this distribution\n",
    "    \"\"\"\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_var) * epsilon\n",
    "\n",
    "def build_vae(data_shape, latent_dim, filters=128):\n",
    "\n",
    "    # Building the encoder - starts with a simple downsampling convolutional network  \n",
    "    encoder = build_conv_net(data_shape, latent_dim*2, filters=filters)\n",
    "    \n",
    "    # Adding special sampling layer that uses the reparametrization trick \n",
    "    z_mean = Dense(latent_dim)(encoder.output)\n",
    "    z_var = Dense(latent_dim)(encoder.output)\n",
    "    z = Sampling()([z_mean, z_var])\n",
    "    \n",
    "    # Connecting the two encoder parts\n",
    "    encoder = tf.keras.Model(inputs=encoder.input, outputs=z)\n",
    "\n",
    "    # Defining the decoder which is a regular upsampling deconvolutional network\n",
    "    decoder = build_deconv_net(latent_dim, activation_out='sigmoid', filters=filters)\n",
    "    vae = tf.keras.Model(inputs=encoder.input, outputs=decoder(z))\n",
    "    \n",
    "    # Adding the special loss term\n",
    "    kl_loss = -0.5 * tf.reduce_sum(z_var - tf.square(z_mean) - tf.exp(z_var) + 1)\n",
    "    vae.add_loss(kl_loss/tf.cast(tf.keras.backend.prod(data_shape), tf.float32))\n",
    "\n",
    "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy')\n",
    "\n",
    "    return encoder, decoder, vae\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the VAE model\n",
    "\n",
    "latent_dim = 32\n",
    "encoder, decoder, vae = build_vae(dataset.shape[1:], latent_dim, filters=128)\n",
    "\n",
    "# Generate random vectors that we will use to sample our latent space\n",
    "for epoch in range(20):\n",
    "    latent_vectors = np.random.randn(9, latent_dim)\n",
    "    vae.fit(x=dataset, y=dataset, epochs=1, batch_size=8)\n",
    "    \n",
    "    images = decoder(latent_vectors)\n",
    "    grid_plot(images, epoch, name='VAE generated images (randomly sampled from the latent space)', n=3, save=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan(data_shape, latent_dim, filters=128, lr=0.0002, beta_1=0.5):\n",
    "    optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=beta_1)\n",
    "\n",
    "    # Usually thew GAN generator has tanh activation function in the output layer\n",
    "    generator = build_deconv_net(latent_dim, activation_out='tanh', filters=filters)\n",
    "    \n",
    "    # Build and compile the discriminator\n",
    "    discriminator = build_conv_net(in_shape=data_shape, out_shape=1, filters=filters) # Single output for binary classification\n",
    "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    # End-to-end GAN model for training the generator\n",
    "    discriminator.trainable = False\n",
    "    true_fake_prediction = discriminator(generator.output)\n",
    "    GAN = tf.keras.Model(inputs=generator.input, outputs=true_fake_prediction)\n",
    "    GAN = tf.keras.models.Sequential([generator, discriminator])\n",
    "    GAN.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "    \n",
    "    return discriminator, generator, GAN\n",
    "\n",
    "def run_generator(generator, n_samples=100):\n",
    "    \"\"\"\n",
    "    Run the generator model and generate n samples of synthetic images using random latent vectors\n",
    "    \"\"\"\n",
    "    latent_dim = generator.layers[0].input_shape[-1]\n",
    "    generator_input = np.random.randn(n_samples, latent_dim)\n",
    "\n",
    "    return generator.predict(generator_input)\n",
    "    \n",
    "\n",
    "def get_batch(generator, dataset, batch_size=64):\n",
    "    \"\"\"\n",
    "    Gets a single batch of samples (X) and labels (y) for the training the discriminator.\n",
    "    One half from the real dataset (labeled as 1s), the other created by the generator model (labeled as 0s).\n",
    "    \"\"\"\n",
    "    batch_size //= 2 # Split evenly among fake and real samples\n",
    "\n",
    "    fake_data = run_generator(generator, n_samples=batch_size)\n",
    "    real_data = dataset[np.random.randint(0, dataset.shape[0], batch_size)]\n",
    "\n",
    "    X = np.concatenate([fake_data, real_data], axis=0)\n",
    "    y = np.concatenate([np.zeros([batch_size, 1]), np.ones([batch_size, 1])], axis=0)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def train_gan(generator, discriminator, gan, dataset, latent_dim, n_epochs=20, batch_size=64):\n",
    "\n",
    "    batches_per_epoch = int(dataset.shape[0] / batch_size / 2)\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(batches_per_epoch):\n",
    "            \n",
    "            # 1) Train discriminator both on real and synthesized images\n",
    "            X, y = get_batch(generator, dataset, batch_size=batch_size)\n",
    "            discriminator_loss = discriminator.train_on_batch(X, y)\n",
    "\n",
    "            # 2) Train generator (note that now the label of synthetic images is reversed to 1)\n",
    "            X_gan = np.random.randn(batch_size, latent_dim)\n",
    "            y_gan = np.ones([batch_size, 1])\n",
    "            generator_loss = gan.train_on_batch(X_gan, y_gan)\n",
    "\n",
    "        noise = np.random.randn(16, latent_dim)\n",
    "        images = generator.predict(noise)\n",
    "        grid_plot(images, epoch, name='GAN generated images', n=3, save=False, scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build and train the model (need around 10 epochs to start seeing some results)\n",
    "\n",
    "latent_dim = 256\n",
    "discriminator, generator, gan = build_gan(dataset.shape[1:], latent_dim, filters=128)\n",
    "dataset_scaled = load_data(scale=True)\n",
    "\n",
    "train_gan(generator, discriminator, gan, dataset_scaled, latent_dim, n_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73a50edc90de9e6c73e4de59c0149da437672aa3b7b7e4f31799c5ff4b4c231b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
