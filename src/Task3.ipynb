{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3: Generative Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDjcuZ_rygha",
        "outputId": "662eb1f3-60dd-4968-8776-69a62b165801"
      },
      "outputs": [],
      "source": [
        "# stdlib\n",
        "import os\n",
        "\n",
        "# pip\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import PIL\n",
        "import tensorflow as tf\n",
        "from keras.layers import Dense, Flatten, Conv2D, Conv2DTranspose, Reshape\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# local\n",
        "from utils import get_dirs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fix directories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DIRS = get_dirs(os.path.abspath('') + os.sep + 'Task3.ipynb')\n",
        "print('\\033[1m' + 'Directories:' + '\\033[0m')\n",
        "for dir_name, path in DIRS.items():\n",
        "    print(f'{dir_name:<7} {path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Fix random seed\n",
        "\n",
        "Setting a random seed to make sure the results can be replicated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4XMNhkbjyghf"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_to_npy(path: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Converts a directory of images to a numpy array.\n",
        "    Returns numpy array of shape (N, H, W, C) where N is the number of images,\n",
        "    H and W are the height and width of the images, and C is the number of channels.\n",
        "    \"\"\"\n",
        "\n",
        "    images = []\n",
        "    for file in os.listdir(path):\n",
        "        if not file.endswith('.png'):\n",
        "            continue\n",
        "        image = PIL.Image.open(os.path.join(path, file))\n",
        "        image = np.array(image)\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "def load_from_npy(path: str, scale: bool = False) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Loads a numpy array from a file.\n",
        "    scale: If False, channels are in the range [0, 1]. If True, channels are in range [-1, 1]\n",
        "    \"\"\"\n",
        "\n",
        "    X = np.load(path)\n",
        "    if scale:\n",
        "        X = (X - 127.5) * 2\n",
        "    return X / 255.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data should be present (unzipped) in the `data` folder.\n",
        "Because this data is a collection of multiple sources, we want to remove duplicates when loading it for the first time.\n",
        "The pruned dataset will be saved as a NumPy array in the `data` folder (cats.npy).\n",
        "We are working with a generative model, so we will not need to use any labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if not os.path.exists(DIRS['data'] + 'cats.npy'):\n",
        "    images = convert_to_npy(path=DIRS['data'] + 'cats/')\n",
        "    # remove duplicates\n",
        "    samples_before = images.shape[0]\n",
        "    images = np.unique(images, axis=0)\n",
        "    samples_after = images.shape[0]\n",
        "    print(f'Removed {samples_before - samples_after} duplicates')\n",
        "    np.save(DIRS['data'] + 'cats.npy', images)\n",
        "else:\n",
        "    images = load_from_npy(DIRS['data'] + 'cats.npy')\n",
        "\n",
        "dataset = images\n",
        "print(f'dataset shape: {dataset.shape}, min: {dataset.min()}, max: {dataset.max()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This function will be called later on, but we also call it once to show that it works. It is used to plot the images in a 3 x 3 grid. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        },
        "id": "el6Icj9gyghg",
        "outputId": "9145dd32-8578-409e-9bef-13d76539dbbd"
      },
      "outputs": [],
      "source": [
        "def grid_plot(\n",
        "    images: np.ndarray,\n",
        "    epoch: int = 0,\n",
        "    name: str = '',\n",
        "    n: int = 3,\n",
        "    save: bool = False,\n",
        "    scale: bool = False\n",
        "    ) -> None:\n",
        "    \"\"\"\n",
        "    Plot a grid of n*n images, note that images.shape[0] must equal n*n.\n",
        "    \"\"\"\n",
        "\n",
        "    if scale:\n",
        "        images = (images + 1) / 2.0\n",
        "    \n",
        "    fig, axes = plt.subplots(n, n, figsize=(n*2, n*2))\n",
        "    for i in range(n * n):\n",
        "        ax = axes[i // n, i % n]\n",
        "        ax.imshow(images[i])\n",
        "        ax.axis('off')\n",
        "    fig.suptitle(f'{name} {epoch}', fontsize=14)\n",
        "    \n",
        "    if save:\n",
        "        fig.savefig(DIRS['figs'] + f'generated_plot_e{epoch+1}03d_f.png')\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "ri_mask = np.random.choice(range(0, dataset.shape[0]), 9, replace=False)\n",
        "random_images = dataset[ri_mask]\n",
        "grid_plot(random_images, name='Cats dataset (64x64x3)', n=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY2JiCWeyghl"
      },
      "source": [
        "## 2.1. Convolutional & De-convolutional\n",
        "\n",
        "Here are the convolutional and de-convolutional neural networks. These function as the basis for the encoding and de-encoding networks of the VAE and GAN. \n",
        " \n",
        "\n",
        "\n",
        "#### Code for building these components:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMljVR5oyghm"
      },
      "outputs": [],
      "source": [
        "def build_conv_net(\n",
        "    in_shape: tuple = (64, 64, 3),\n",
        "    out_shape: int = 512,\n",
        "    n_downsampling_layers: int = 4,\n",
        "    filters: int = 128,\n",
        "    out_activation: str = 'sigmoid'\n",
        "    ) -> tf.keras.Sequential:\n",
        "    \"\"\"\n",
        "    Build a basic convolutional network\n",
        "    \"\"\"\n",
        "    \n",
        "    model = tf.keras.Sequential()\n",
        "    default_args = dict(\n",
        "        kernel_size = (3, 3),\n",
        "        strides = (2, 2),\n",
        "        padding = 'same',\n",
        "        activation = 'relu'\n",
        "    )\n",
        "\n",
        "    model.add(Conv2D(\n",
        "        input_shape = in_shape,\n",
        "        **default_args,\n",
        "        filters = filters\n",
        "    ))\n",
        "\n",
        "    for _ in range(n_downsampling_layers):\n",
        "        model.add(Conv2D(\n",
        "            **default_args,\n",
        "            filters = filters\n",
        "        ))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(out_shape, activation=out_activation) )\n",
        "    model.summary()\n",
        "    \n",
        "    return model\n",
        "\n",
        "\n",
        "def build_deconv_net(\n",
        "    latent_dim: int = 512,\n",
        "    n_upsampling_layers = 4,\n",
        "    filters = 128,\n",
        "    activation_out = 'sigmoid'\n",
        "    ) -> tf.keras.Sequential:\n",
        "    \"\"\"\n",
        "    Build a deconvolutional network for decoding/upscaling latent vectors\n",
        "    \"\"\"\n",
        "\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Dense(4 * 4 * 64, input_dim=latent_dim)) \n",
        "    model.add(Reshape((4, 4, 64))) # This matches the output size of the downsampling architecture\n",
        "    default_args = dict(\n",
        "        kernel_size = (3, 3),\n",
        "        strides = (2, 2),\n",
        "        padding = 'same',\n",
        "        activation = 'relu'\n",
        "    )\n",
        "    \n",
        "    for _ in range(n_upsampling_layers):\n",
        "        model.add(Conv2DTranspose(**default_args, filters=filters))\n",
        "\n",
        "    # This last convolutional layer converts back to 3 channel RGB image\n",
        "    model.add(Conv2D(filters=3, kernel_size=(3,3), activation=activation_out, padding='same'))\n",
        "    model.summary()\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWInkZXdyghv"
      },
      "source": [
        "--- \n",
        "---\n",
        "\n",
        "\n",
        "## 2. 2. Variational Autoencoders (VAEs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyGbnibCyghw"
      },
      "outputs": [],
      "source": [
        "class Sampling(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Custom layer for the variational autoencoder\n",
        "    It takes two vectors as input - one for means and other for variances of the latent variables described by a multimodal gaussian\n",
        "    Its output is a latent vector randomly sampled from this distribution\n",
        "    \"\"\"\n",
        "    def call(self, inputs):\n",
        "        z_mean, z_var = inputs\n",
        "        batch = tf.shape(z_mean)[0]\n",
        "        dim = tf.shape(z_mean)[1]\n",
        "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
        "        return z_mean + tf.exp(0.5 * z_var) * epsilon\n",
        "\n",
        "def build_vae(data_shape, latent_dim, filters=128):\n",
        "\n",
        "    # Building the encoder - starts with a simple downsampling convolutional network  \n",
        "    encoder = build_conv_net(data_shape, latent_dim*2, filters=filters)\n",
        "        \n",
        "    \n",
        "    # Adding special sampling layer that uses the reparametrization trick \n",
        "    z_mean = Dense(latent_dim)(encoder.output)\n",
        "    z_var = Dense(latent_dim)(encoder.output)\n",
        "    z = Sampling()([z_mean, z_var])\n",
        "    \n",
        "    # Connecting the two encoder parts\n",
        "    encoder = tf.keras.Model(inputs=encoder.input, outputs=z)\n",
        "\n",
        "    # Defining the decoder which is a regular upsampling deconvolutional network\n",
        "    decoder = build_deconv_net(latent_dim, activation_out='sigmoid', filters=filters)\n",
        "    vae = tf.keras.Model(inputs=encoder.input, outputs=decoder(z))\n",
        "    \n",
        "    # Adding the special loss term\n",
        "    kl_loss = -0.5 * tf.reduce_sum(z_var - tf.square(z_mean) - tf.exp(z_var) + 1)\n",
        "    vae.add_loss(kl_loss/tf.cast(tf.keras.backend.prod(data_shape), tf.float32))\n",
        "\n",
        "    vae.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss='binary_crossentropy')\n",
        "\n",
        "    return encoder, decoder, vae\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training the VAE model saving the interpolated vectors\n",
        "epochs = 10\n",
        "latent_dim = 32\n",
        "encoder, decoder, vae = build_vae(dataset.shape[1:], latent_dim, filters=128)\n",
        "all_vectors = np.zeros((10,9,32))\n",
        "\n",
        "# Generate random vectors that we will use to sample our latent space. The output is a grid image of the normal latent vectors.\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    latent_vectors = np.random.randn(9, latent_dim)\n",
        "    vae.fit(x=dataset, y=dataset, epochs=1, batch_size=8)\n",
        "    ip_vectors = np.random.randn().zeros((len(latent_vectors), latent_dim))\n",
        "    \n",
        "    for i in range(9):\n",
        "        if i != 8:\n",
        "            ip_vectors = np.random.randn()[i][:] = (latent_vectors[i] + latent_vectors[i + 1]) / 2\n",
        "        else:\n",
        "            ip_vectors = np.random.randn()[i][:] = (latent_vectors[i] + latent_vectors[0]) / 2\n",
        "    \n",
        "    all_vectors[epoch] = ip_vectors = np.random.randn()\n",
        " \n",
        "    images = decoder(latent_vectors)\n",
        "    grid_plot(images, epoch, name='VAE generated images interpolated', n=3, save=False)\n",
        "        \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Display the different interpolated images, these are saved during fitting and decoded after the fitting has ended. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(all_vectors.shape[0]):\n",
        "    images = decoder(all_vectors[i])\n",
        "    grid_plot(images, i, name='VAE generated images interpolated', n=3, save=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkT8Wmc_ygh3"
      },
      "source": [
        "---\n",
        "\n",
        "## 2.3 Generative Adversarial Networks (GANs)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5J6CxJ9ygh4"
      },
      "outputs": [],
      "source": [
        "def build_gan(\n",
        "    data_shape,\n",
        "    latent_dim,\n",
        "    filters = 128,\n",
        "    lr = 0.0002,\n",
        "    beta_1 = 0.5\n",
        "    ):\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=lr, beta_1=beta_1)\n",
        "\n",
        "    # Usually thew GAN generator has tanh activation function in the output layer\n",
        "    generator = build_deconv_net(latent_dim, activation_out='tanh', filters=filters)\n",
        "    \n",
        "    # Build and compile the discriminator\n",
        "    discriminator = build_conv_net(in_shape=data_shape, out_shape=1, filters=filters) # Single output for binary classification\n",
        "    discriminator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    # End-to-end GAN model for training the generator\n",
        "    discriminator.trainable = False\n",
        "    true_fake_prediction = discriminator(generator.output)\n",
        "    GAN = tf.keras.Model(inputs=generator.input, outputs=true_fake_prediction)\n",
        "    GAN = tf.keras.models.Sequential([generator, discriminator])\n",
        "    GAN.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "    \n",
        "    return discriminator, generator, GAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD3ipOOiygh8"
      },
      "source": [
        "### Definining custom functions for training your GANs\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_generator(generator, n_samples=100):\n",
        "    \"\"\"\n",
        "    Run the generator model and generate n samples of synthetic images using random latent vectors\n",
        "    \"\"\"\n",
        "    latent_dim = generator.layers[0].input_shape[-1]\n",
        "    generator_input = np.random.randn(n_samples, latent_dim)\n",
        "\n",
        "    return generator.predict(generator_input)\n",
        "\n",
        "def get_batch(generator, dataset, batch_size=64):\n",
        "    \"\"\"\n",
        "    Gets a single batch of samples (X) and labels (y) for the training the discriminator.\n",
        "    One half from the real dataset (labeled as 1s), the other created by the generator model (labeled as 0s).\n",
        "    \"\"\"\n",
        "    batch_size //= 2 # Split evenly among fake and real samples\n",
        "\n",
        "    fake_data = run_generator(generator, n_samples=batch_size)\n",
        "    real_data = dataset[np.random.randint(0, dataset.shape[0], batch_size)]\n",
        "\n",
        "    X = np.concatenate([fake_data, real_data], axis=0)\n",
        "    y = np.concatenate([np.zeros([batch_size, 1]), np.ones([batch_size, 1])], axis=0)\n",
        "\n",
        "    return X, y\n",
        "\n",
        "def train_gan_interpolation(generator, discriminator, gan, dataset, latent_dim, n_epochs=10, batch_size=64):\n",
        "\n",
        "    df = pd.DataFrame(columns=['discriminator_loss', 'generator_loss'])\n",
        "    batches_per_epoch = int(dataset.shape[0] / batch_size / 2)\n",
        "    all_noises = np.empty((n_epochs, 16, 256))\n",
        "    for epoch in range(n_epochs):\n",
        "        d_loss, g_loss = np.zeros(batches_per_epoch), np.zeros(batches_per_epoch)\n",
        "        for batch in range(batches_per_epoch):\n",
        "            \n",
        "            # 1) Train discriminator both on real and synthesized images\n",
        "            X, y = get_batch(generator, dataset, batch_size=batch_size)\n",
        "            discriminator_loss = discriminator.train_on_batch(X, y)\n",
        "            d_loss[batch] = discriminator_loss\n",
        "\n",
        "            # 2) Train generator \n",
        "            X_gan = np.random.randn(batch_size, latent_dim)\n",
        "            y_gan = np.ones([batch_size, 1])\n",
        "            generator_loss = gan.train_on_batch(X_gan, y_gan)\n",
        "            g_loss[batch] = generator_loss\n",
        "\n",
        "        mean_d_loss, mean_g_loss = np.mean(d_loss), np.mean(g_loss)\n",
        "        print(mean_d_loss, mean_g_loss)\n",
        "        df.loc[epoch] = [mean_d_loss, mean_g_loss]\n",
        "        noise = np.random.randn(16, latent_dim)\n",
        "        it_noises = np.zeros((len(noise), latent_dim))\n",
        "\n",
        "        for i in range(16):\n",
        "            if i != 15:\n",
        "                it_noises[i][:] = (noise[i] + noise[i + 1]) / 2\n",
        "            else:\n",
        "                it_noises[i][:] = (noise[i] + noise[0]) / 2\n",
        "        \n",
        "        all_noises[epoch] = it_noises\n",
        "        images = generator.predict(noise)\n",
        "        grid_plot(images, epoch, name='GAN generated images', n=3, save=False, scale=True)\n",
        "\n",
        "    return df, all_noises, generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VpMsdygAygiB",
        "tags": []
      },
      "outputs": [],
      "source": [
        "## Build and train the model (need around 20 epochs to start seeing some results)\n",
        "\n",
        "latent_dim = 256\n",
        "discriminator, generator, gan = build_gan(dataset.shape[1:], latent_dim, filters=128)\n",
        "dataset_scaled = dataset * 2 - 1  # Scale to [-1, 1] for tanh activation function\n",
        "\n",
        "loss_df_i, all_noises, gen = train_gan_interpolation(generator, discriminator, gan, dataset_scaled, latent_dim, n_epochs=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the loss of the generator and discriminator. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "\n",
        "ax.plot(loss_df_i['discriminator_loss'], label='Discriminator')\n",
        "ax.plot(loss_df_i['generator_loss'], label='Generator')\n",
        "ax.set_title('GAN loss')\n",
        "ax.set_xlabel('Epoch')\n",
        "ax.set_ylabel('Loss')\n",
        "ax.legend()\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the interpolated vectors, using the same technique as before.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for i in range(20):\n",
        "    images =  generator.predict(all_noises[i])\n",
        "    grid_plot(images, i, name='GAN generated images interpolated', n=3, save=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.8 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "73a50edc90de9e6c73e4de59c0149da437672aa3b7b7e4f31799c5ff4b4c231b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
